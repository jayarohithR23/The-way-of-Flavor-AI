THE WAY OF FLAVOR AI (味の道AI) — DEMO EXPLAINATION

Purpose
- Help non‑cooks “cook delicious food” using what they have at home.
- Minimum‑friction MVP: detect ingredients → suggest authentic Indian recipes → view steps.

How this maps to the company’s tech‑demo brief
- Use case: “I want to cook delicious food.” ✓
- Technical areas covered:
  - Image Classification (lightweight): local heuristic detects ingredients from image.
  - Natural Language: bilingual UI (English/Japanese) across the app.
  - Generation AI (optional): live translation endpoint /api/translate (enable with OPENAI_API_KEY).
- MVP level: Landing page → Start Cooking → detect → results → read recipe. No accounts, runs locally with MongoDB.
- Minimum instructions: README provides 1‑page setup (env, run, toggles).

What the MVP does today (works out‑of‑the‑box)
1) Ingredient detection (local heuristic):
   - Upload a photo; backend returns a few likely ingredients (no external APIs).
   - UI shows a toast like “Detected 6 ingredients with 87% confidence” (UX hint value).
2) Smart recipe search (local‑first):
   - Queries MongoDB collection recipe_assistant.recipes using the detected or typed ingredients.
   - International fallback is disabled by default (ENABLE_EXTERNAL=false) to keep the demo focused.
3) Bilingual UI:
   - Instant EN/JA toggle for headings, labels, and recipe fields (title_jp, ingredients_jp, instructions_jp).
4) Clean UX:
   - Minimal landing page, one primary CTA (“Start Cooking”), responsive layout.

Optional enhancements you can showcase (toggleable / small scope)
A) Live translation demo (Generation AI)
   - What: Send sample text to /api/translate to show live EN↔JA translation.
   - Enable: Add OPENAI_API_KEY to backend/.env and restart backend.
   - Value: Demonstrates generative AI integration without changing core flow.

B) Ingredient normalization (NLP Lite)
   - What: Map synonyms/plurals (tomatoes → tomato, chick peas → chickpea) before querying MongoDB.
   - Value: Improves search precision/recall and user outcomes with minimal UI change.

C) Step summarization (NLP)
   - What: Generate a short TL;DR for instructions or auto‑extract prep/cook time if missing.
   - Value: Faster comprehension; good “wow” for non‑cooks.

D) Keyword tagging (NLP)
   - What: Auto‑tag recipes (e.g., “spicy”, “vegan”, “gluten‑free”, “one‑pot”) from instructions text.
   - Value: Better filtering chips and curated collections.

E) Simple natural‑language search (NLP)
   - What: Parse queries like “easy chicken dinner under 30 mins” → difficulty=easy, ingredient=chicken, time<=30.
   - Value: Conversational discovery with light rules or a small LLM prompt.

How to enable OpenAI later (only if you want the translation demo)
1) backend/.env → add:
   OPENAI_API_KEY=your_actual_key
2) Restart backend:
   cd backend && npm run dev
3) Test:
   curl -X POST http://localhost:5000/api/translate -H "Content-Type: application/json" -d '{"text":"Butter Chicken","targetLanguage":"ja"}'

Demo script (suggested flow on stage)
1) Home page → click “Start Cooking”.
2) Drag an ingredient photo → show toast; verify detected ingredients chips.
3) Point at result banner (e.g., “Found 26 Indian recipes + 0 international recipes”).
4) Open a recipe → demonstrate EN↔JA toggle.
5) (If OpenAI enabled) Click “Translate demo” button or run curl to show live translation.

Deployment/ops notes
- Local‑first: no external API calls by default (besides MongoDB).
- External recipes are off unless ENABLE_EXTERNAL=true.
- Seed file backend/data/indian_recipes.json initializes DB on first run; can be kept for portability.

Talking points (1‑minute pitch)
- “This MVP helps anyone cook with what they already have. Snap a photo, we detect ingredients, and instantly surface authentic Indian recipes. Everything runs locally by default, and we can flip a switch to add live translation or conversational search when needed.”
